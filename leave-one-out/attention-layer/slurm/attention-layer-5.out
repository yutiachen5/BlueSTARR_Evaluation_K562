dcc-majoroslab-gpu-04
Mon Aug 12 09:21:56 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:13:00.0 Off |                  Off |
| 30%   24C    P8             26W /  300W |       2MiB /  49140MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Using MSE loss function with naive estimator
loading data
number of fasta seq:  2160271
number of counts:  2160271
number of fasta seq:  207943
number of counts:  207943
number of fasta seq:  231129
number of counts:  231129
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_1 (InputLayer)        [(None, 300, 4)]             0         []                            
                                                                                                  
 conv1d (Conv1D)             (None, 300, 256)             7424      ['input_1[0][0]']             
                                                                                                  
 batch_normalization (Batch  (None, 300, 256)             1024      ['conv1d[0][0]']              
 Normalization)                                                                                   
                                                                                                  
 activation (Activation)     (None, 300, 256)             0         ['batch_normalization[0][0]'] 
                                                                                                  
 sine_position_encoding (Si  (None, 300, 256)             0         ['activation[0][0]']          
 nePositionEncoding)                                                                              
                                                                                                  
 tf.__operators__.add (TFOp  (None, 300, 256)             0         ['activation[0][0]',          
 Lambda)                                                             'sine_position_encoding[0][0]
                                                                    ']                            
                                                                                                  
 layer_normalization (Layer  (None, 300, 256)             512       ['tf.__operators__.add[0][0]']
 Normalization)                                                                                   
                                                                                                  
 transformer_encoder (Trans  (None, 300, 256)             789760    ['layer_normalization[0][0]'] 
 formerEncoder)                                                                                   
                                                                                                  
 layer_normalization_1 (Lay  (None, 300, 256)             512       ['transformer_encoder[0][0]'] 
 erNormalization)                                                                                 
                                                                                                  
 transformer_encoder_1 (Tra  (None, 300, 256)             789760    ['layer_normalization_1[0][0]'
 nsformerEncoder)                                                   ]                             
                                                                                                  
 layer_normalization_2 (Lay  (None, 300, 256)             512       ['transformer_encoder_1[0][0]'
 erNormalization)                                                   ]                             
                                                                                                  
 transformer_encoder_2 (Tra  (None, 300, 256)             789760    ['layer_normalization_2[0][0]'
 nsformerEncoder)                                                   ]                             
                                                                                                  
 layer_normalization_3 (Lay  (None, 300, 256)             512       ['transformer_encoder_2[0][0]'
 erNormalization)                                                   ]                             
                                                                                                  
 transformer_encoder_3 (Tra  (None, 300, 256)             789760    ['layer_normalization_3[0][0]'
 nsformerEncoder)                                                   ]                             
                                                                                                  
 layer_normalization_4 (Lay  (None, 300, 256)             512       ['transformer_encoder_3[0][0]'
 erNormalization)                                                   ]                             
                                                                                                  
 transformer_encoder_4 (Tra  (None, 300, 256)             789760    ['layer_normalization_4[0][0]'
 nsformerEncoder)                                                   ]                             
                                                                                                  
 average_pooling1d (Average  (None, 1, 256)               0         ['transformer_encoder_4[0][0]'
 Pooling1D)                                                         ]                             
                                                                                                  
 K562 (Dense)                (None, 1, 1)                 257       ['average_pooling1d[0][0]']   
                                                                                                  
==================================================================================================
Total params: 3960065 (15.11 MB)
Trainable params: 3959553 (15.10 MB)
Non-trainable params: 512 (2.00 KB)
__________________________________________________________________________________________________
Training...
Training set: (2160271, 300, 4)
Epoch 1/200
33755/33755 - 12626s - loss: 0.0712 - val_loss: 0.0575 - 12626s/epoch - 374ms/step
Epoch 2/200
33755/33755 - 11951s - loss: 0.0616 - val_loss: 0.0574 - 11951s/epoch - 354ms/step
Epoch 3/200
33755/33755 - 11947s - loss: 0.0616 - val_loss: 0.0572 - 11947s/epoch - 354ms/step
Epoch 4/200
33755/33755 - 12100s - loss: 0.0616 - val_loss: 0.0577 - 12100s/epoch - 358ms/step
Epoch 5/200
33755/33755 - 11975s - loss: 0.0616 - val_loss: 0.0573 - 11975s/epoch - 355ms/step
Epoch 6/200
33755/33755 - 11869s - loss: 0.0616 - val_loss: 0.0571 - 11869s/epoch - 352ms/step
Epoch 7/200
33755/33755 - 11891s - loss: 0.0616 - val_loss: 0.0570 - 11891s/epoch - 352ms/step
Epoch 8/200
33755/33755 - 11904s - loss: 0.0616 - val_loss: 0.0571 - 11904s/epoch - 353ms/step
Epoch 9/200
33755/33755 - 11844s - loss: 0.0616 - val_loss: 0.0573 - 11844s/epoch - 351ms/step
Epoch 10/200
33755/33755 - 12085s - loss: 0.0616 - val_loss: 0.0571 - 12085s/epoch - 358ms/step
Epoch 11/200
33755/33755 - 12080s - loss: 0.0616 - val_loss: 0.0569 - 12080s/epoch - 358ms/step
Epoch 12/200
33755/33755 - 11967s - loss: 0.0616 - val_loss: 0.0570 - 11967s/epoch - 355ms/step
Epoch 13/200
33755/33755 - 11976s - loss: 0.0616 - val_loss: 0.0571 - 11976s/epoch - 355ms/step
Epoch 14/200
33755/33755 - 11964s - loss: 0.0616 - val_loss: 0.0573 - 11964s/epoch - 354ms/step
Epoch 15/200
33755/33755 - 11994s - loss: 0.0616 - val_loss: 0.0576 - 11994s/epoch - 355ms/step
Epoch 16/200
33755/33755 - 12040s - loss: 0.0616 - val_loss: 0.0570 - 12040s/epoch - 357ms/step
Epoch 17/200
33755/33755 - 11871s - loss: 0.0616 - val_loss: 0.0569 - 11871s/epoch - 352ms/step
Epoch 18/200
33755/33755 - 12054s - loss: 0.0616 - val_loss: 0.0571 - 12054s/epoch - 357ms/step
Epoch 19/200
33755/33755 - 12051s - loss: 0.0616 - val_loss: 0.0569 - 12051s/epoch - 357ms/step
Epoch 20/200
33755/33755 - 12032s - loss: 0.0616 - val_loss: 0.0571 - 12032s/epoch - 356ms/step
Epoch 21/200
33755/33755 - 12017s - loss: 0.0616 - val_loss: 0.0569 - 12017s/epoch - 356ms/step
Epoch 22/200
33755/33755 - 12028s - loss: 0.0616 - val_loss: 0.0572 - 12028s/epoch - 356ms/step
Epoch 23/200
33755/33755 - 11773s - loss: 0.0616 - val_loss: 0.0570 - 11773s/epoch - 349ms/step
Epoch 24/200
33755/33755 - 12167s - loss: 0.0616 - val_loss: 0.0575 - 12167s/epoch - 360ms/step
Epoch 25/200
33755/33755 - 11987s - loss: 0.0616 - val_loss: 0.0572 - 11987s/epoch - 355ms/step
Epoch 26/200
33755/33755 - 11883s - loss: 0.0616 - val_loss: 0.0570 - 11883s/epoch - 352ms/step
Epoch 27/200
33755/33755 - 11897s - loss: 0.0616 - val_loss: 0.0571 - 11897s/epoch - 352ms/step
{'loss': [0.0711943507194519, 0.061573415994644165, 0.06156661733984947, 0.06156313046813011, 0.061564572155475616, 0.06156051903963089, 0.06156184524297714, 0.061559565365314484, 0.06155925989151001, 0.06156139448285103, 0.06155943498015404, 0.06156240776181221, 0.06156226992607117, 0.06156313046813011, 0.06156318262219429, 0.06156649440526962, 0.06156402826309204, 0.06156865879893303, 0.06156974285840988, 0.06157483533024788, 0.06156766787171364, 0.06156959757208824, 0.06157048046588898, 0.06156860664486885, 0.06157011166214943, 0.0615728385746479, 0.061572954058647156], 'val_loss': [0.05748588219285011, 0.057381074875593185, 0.05724988877773285, 0.057680316269397736, 0.05725181847810745, 0.05708948150277138, 0.056984126567840576, 0.057050373405218124, 0.05732736364006996, 0.05706057325005531, 0.05692838504910469, 0.05696100741624832, 0.057104986160993576, 0.05727853253483772, 0.057564616203308105, 0.057045869529247284, 0.056916721165180206, 0.0570613257586956, 0.05694735422730446, 0.057103242725133896, 0.05692935362458229, 0.05724833905696869, 0.057027652859687805, 0.057454705238342285, 0.057180266827344894, 0.05698234215378761, 0.057138845324516296]}
Done training
loss [0.0711943507194519, 0.061573415994644165, 0.06156661733984947, 0.06156313046813011, 0.061564572155475616, 0.06156051903963089, 0.06156184524297714, 0.061559565365314484, 0.06155925989151001, 0.06156139448285103, 0.06155943498015404, 0.06156240776181221, 0.06156226992607117, 0.06156313046813011, 0.06156318262219429, 0.06156649440526962, 0.06156402826309204, 0.06156865879893303, 0.06156974285840988, 0.06157483533024788, 0.06156766787171364, 0.06156959757208824, 0.06157048046588898, 0.06156860664486885, 0.06157011166214943, 0.0615728385746479, 0.061572954058647156]
val_loss [0.05748588219285011, 0.057381074875593185, 0.05724988877773285, 0.057680316269397736, 0.05725181847810745, 0.05708948150277138, 0.056984126567840576, 0.057050373405218124, 0.05732736364006996, 0.05706057325005531, 0.05692838504910469, 0.05696100741624832, 0.057104986160993576, 0.05727853253483772, 0.057564616203308105, 0.057045869529247284, 0.056916721165180206, 0.0570613257586956, 0.05694735422730446, 0.057103242725133896, 0.05692935362458229, 0.05724833905696869, 0.057027652859687805, 0.057454705238342285, 0.057180266827344894, 0.05698234215378761, 0.057138845324516296]
