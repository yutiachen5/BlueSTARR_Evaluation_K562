Using MSE loss function with naive estimator
loading data
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 600, 4)]          0         
                                                                 
 conv1d (Conv1D)             (None, 600, 1024)         33792     
                                                                 
 batch_normalization (Batch  (None, 600, 1024)         4096      
 Normalization)                                                  
                                                                 
 activation (Activation)     (None, 600, 1024)         0         
                                                                 
 dropout (Dropout)           (None, 600, 1024)         0         
                                                                 
 conv1d_1 (Conv1D)           (None, 600, 683)          8393387   
                                                                 
 batch_normalization_1 (Bat  (None, 600, 683)          2732      
 chNormalization)                                                
                                                                 
 activation_1 (Activation)   (None, 600, 683)          0         
                                                                 
 dropout_1 (Dropout)         (None, 600, 683)          0         
                                                                 
 conv1d_2 (Conv1D)           (None, 600, 456)          5606520   
                                                                 
 batch_normalization_2 (Bat  (None, 600, 456)          1824      
 chNormalization)                                                
                                                                 
 activation_2 (Activation)   (None, 600, 456)          0         
                                                                 
 dropout_2 (Dropout)         (None, 600, 456)          0         
                                                                 
 conv1d_3 (Conv1D)           (None, 600, 304)          3743152   
                                                                 
 batch_normalization_3 (Bat  (None, 600, 304)          1216      
 chNormalization)                                                
                                                                 
 activation_3 (Activation)   (None, 600, 304)          0         
                                                                 
 dropout_3 (Dropout)         (None, 600, 304)          0         
                                                                 
 conv1d_4 (Conv1D)           (None, 600, 203)          2530395   
                                                                 
 batch_normalization_4 (Bat  (None, 600, 203)          812       
 chNormalization)                                                
                                                                 
 activation_4 (Activation)   (None, 600, 203)          0         
                                                                 
 dropout_4 (Dropout)         (None, 600, 203)          0         
                                                                 
 conv1d_5 (Conv1D)           (None, 600, 135)          1671840   
                                                                 
 batch_normalization_5 (Bat  (None, 600, 135)          540       
 chNormalization)                                                
                                                                 
 activation_5 (Activation)   (None, 600, 135)          0         
                                                                 
 dropout_5 (Dropout)         (None, 600, 135)          0         
                                                                 
 conv1d_6 (Conv1D)           (None, 600, 90)           1117890   
                                                                 
 batch_normalization_6 (Bat  (None, 600, 90)           360       
 chNormalization)                                                
                                                                 
 activation_6 (Activation)   (None, 600, 90)           0         
                                                                 
 dropout_6 (Dropout)         (None, 600, 90)           0         
                                                                 
 conv1d_7 (Conv1D)           (None, 600, 60)           739860    
                                                                 
 batch_normalization_7 (Bat  (None, 600, 60)           240       
 chNormalization)                                                
                                                                 
 activation_7 (Activation)   (None, 600, 60)           0         
                                                                 
 dropout_7 (Dropout)         (None, 600, 60)           0         
                                                                 
 conv1d_8 (Conv1D)           (None, 600, 40)           494440    
                                                                 
 batch_normalization_8 (Bat  (None, 600, 40)           160       
 chNormalization)                                                
                                                                 
 activation_8 (Activation)   (None, 600, 40)           0         
                                                                 
 dropout_8 (Dropout)         (None, 600, 40)           0         
                                                                 
 conv1d_9 (Conv1D)           (None, 600, 27)           324027    
                                                                 
 batch_normalization_9 (Bat  (None, 600, 27)           108       
 chNormalization)                                                
                                                                 
 activation_9 (Activation)   (None, 600, 27)           0         
                                                                 
 max_pooling1d (MaxPooling1  (None, 1, 27)             0         
 D)                                                              
                                                                 
 K562 (Dense)                (None, 1, 1)              28        
                                                                 
=================================================================
Total params: 24667419 (94.10 MB)
Trainable params: 24661375 (94.08 MB)
Non-trainable params: 6044 (23.61 KB)
_________________________________________________________________
Training...
Training set: (700000, 600, 4)
Epoch 1/200
10938/10938 - 2053s - loss: 0.2876 - val_loss: 0.2817 - 2053s/epoch - 188ms/step
Epoch 2/200
10938/10938 - 2023s - loss: 0.2792 - val_loss: 0.2798 - 2023s/epoch - 185ms/step
Epoch 3/200
10938/10938 - 2022s - loss: 0.2742 - val_loss: 0.2836 - 2022s/epoch - 185ms/step
Epoch 4/200
10938/10938 - 2035s - loss: 0.2691 - val_loss: 0.2773 - 2035s/epoch - 186ms/step
Epoch 5/200
10938/10938 - 1960s - loss: 0.2633 - val_loss: 0.3008 - 1960s/epoch - 179ms/step
Epoch 6/200
10938/10938 - 1884s - loss: 0.2547 - val_loss: 0.2776 - 1884s/epoch - 172ms/step
Epoch 7/200
10938/10938 - 1889s - loss: 0.2427 - val_loss: 0.2811 - 1889s/epoch - 173ms/step
Epoch 8/200
10938/10938 - 1869s - loss: 0.2290 - val_loss: 0.2416 - 1869s/epoch - 171ms/step
Epoch 9/200
10938/10938 - 1903s - loss: 0.2166 - val_loss: 0.2087 - 1903s/epoch - 174ms/step
Epoch 10/200
10938/10938 - 1916s - loss: 0.2056 - val_loss: 0.3039 - 1916s/epoch - 175ms/step
Epoch 11/200
10938/10938 - 1929s - loss: 0.1960 - val_loss: 0.1746 - 1929s/epoch - 176ms/step
Epoch 12/200
10938/10938 - 1889s - loss: 0.1882 - val_loss: 0.1985 - 1889s/epoch - 173ms/step
Epoch 13/200
10938/10938 - 1876s - loss: 0.1807 - val_loss: 0.1712 - 1876s/epoch - 171ms/step
Epoch 14/200
10938/10938 - 1916s - loss: 0.1750 - val_loss: 0.1803 - 1916s/epoch - 175ms/step
Epoch 15/200
10938/10938 - 1790s - loss: 0.1690 - val_loss: 0.2097 - 1790s/epoch - 164ms/step
Epoch 16/200
10938/10938 - 1821s - loss: 0.1641 - val_loss: 0.1935 - 1821s/epoch - 166ms/step
Epoch 17/200
10938/10938 - 1909s - loss: 0.1595 - val_loss: 0.1628 - 1909s/epoch - 175ms/step
Epoch 18/200
10938/10938 - 1900s - loss: 0.1552 - val_loss: 0.2477 - 1900s/epoch - 174ms/step
Epoch 19/200
10938/10938 - 1919s - loss: 0.1516 - val_loss: 0.2060 - 1919s/epoch - 175ms/step
Epoch 20/200
10938/10938 - 1886s - loss: 0.1485 - val_loss: 0.1946 - 1886s/epoch - 172ms/step
Epoch 21/200
10938/10938 - 1907s - loss: 0.1449 - val_loss: 0.2330 - 1907s/epoch - 174ms/step
Epoch 22/200
10938/10938 - 1903s - loss: 0.1422 - val_loss: 0.1863 - 1903s/epoch - 174ms/step
Epoch 23/200
10938/10938 - 1883s - loss: 0.1397 - val_loss: 0.1745 - 1883s/epoch - 172ms/step
Epoch 24/200
10938/10938 - 1884s - loss: 0.1368 - val_loss: 0.2490 - 1884s/epoch - 172ms/step
Epoch 25/200
10938/10938 - 1908s - loss: 0.1344 - val_loss: 0.2602 - 1908s/epoch - 174ms/step
Epoch 26/200
10938/10938 - 1898s - loss: 0.1322 - val_loss: 0.1728 - 1898s/epoch - 174ms/step
Epoch 27/200
10938/10938 - 1869s - loss: 0.1300 - val_loss: 0.1455 - 1869s/epoch - 171ms/step
Epoch 28/200
10938/10938 - 1854s - loss: 0.1284 - val_loss: 0.1607 - 1854s/epoch - 170ms/step
Epoch 29/200
10938/10938 - 1870s - loss: 0.1260 - val_loss: 0.1740 - 1870s/epoch - 171ms/step
Epoch 30/200
10938/10938 - 1928s - loss: 0.1243 - val_loss: 0.2261 - 1928s/epoch - 176ms/step
Epoch 31/200
10938/10938 - 1877s - loss: 0.1227 - val_loss: 0.1937 - 1877s/epoch - 172ms/step
Epoch 32/200
10938/10938 - 1795s - loss: 0.1211 - val_loss: 0.2054 - 1795s/epoch - 164ms/step
Epoch 33/200
10938/10938 - 1831s - loss: 0.1198 - val_loss: 0.1878 - 1831s/epoch - 167ms/step
Epoch 34/200
10938/10938 - 1837s - loss: 0.1182 - val_loss: 0.2394 - 1837s/epoch - 168ms/step
Epoch 35/200
10938/10938 - 1880s - loss: 0.1168 - val_loss: 0.2191 - 1880s/epoch - 172ms/step
Epoch 36/200
10938/10938 - 1863s - loss: 0.1157 - val_loss: 0.1945 - 1863s/epoch - 170ms/step
Epoch 37/200
10938/10938 - 1892s - loss: 0.1143 - val_loss: 0.2081 - 1892s/epoch - 173ms/step
{'loss': [0.2876410484313965, 0.2791844606399536, 0.27420130372047424, 0.2691173553466797, 0.26332172751426697, 0.25466272234916687, 0.24273262917995453, 0.2290046215057373, 0.21655890345573425, 0.20556649565696716, 0.19596388936042786, 0.18820598721504211, 0.18068666756153107, 0.17500759661197662, 0.16896480321884155, 0.16408969461917877, 0.1595422476530075, 0.15523838996887207, 0.15163053572177887, 0.14845497906208038, 0.14490172266960144, 0.1421581208705902, 0.1396574079990387, 0.13679368793964386, 0.1344086229801178, 0.13224637508392334, 0.13000144064426422, 0.12840640544891357, 0.12603743374347687, 0.12425778061151505, 0.12267433851957321, 0.12108579277992249, 0.11982236802577972, 0.11815457791090012, 0.11677553504705429, 0.11567357927560806, 0.11433912813663483], 'val_loss': [0.28174149990081787, 0.2797890305519104, 0.28358978033065796, 0.27727338671684265, 0.3007770776748657, 0.2775935232639313, 0.28110551834106445, 0.24156667292118073, 0.20867107808589935, 0.3039432168006897, 0.17458944022655487, 0.19854313135147095, 0.17118075489997864, 0.180263951420784, 0.20967070758342743, 0.193481907248497, 0.16275599598884583, 0.24767570197582245, 0.20603327453136444, 0.1946435123682022, 0.23297514021396637, 0.1863243728876114, 0.17451566457748413, 0.24896660447120667, 0.26021212339401245, 0.17278990149497986, 0.14554882049560547, 0.16067463159561157, 0.17398634552955627, 0.2261389046907425, 0.19369851052761078, 0.20536503195762634, 0.1877540796995163, 0.23937729001045227, 0.21908843517303467, 0.19453273713588715, 0.20810450613498688]}
Done training
loss [0.2876410484313965, 0.2791844606399536, 0.27420130372047424, 0.2691173553466797, 0.26332172751426697, 0.25466272234916687, 0.24273262917995453, 0.2290046215057373, 0.21655890345573425, 0.20556649565696716, 0.19596388936042786, 0.18820598721504211, 0.18068666756153107, 0.17500759661197662, 0.16896480321884155, 0.16408969461917877, 0.1595422476530075, 0.15523838996887207, 0.15163053572177887, 0.14845497906208038, 0.14490172266960144, 0.1421581208705902, 0.1396574079990387, 0.13679368793964386, 0.1344086229801178, 0.13224637508392334, 0.13000144064426422, 0.12840640544891357, 0.12603743374347687, 0.12425778061151505, 0.12267433851957321, 0.12108579277992249, 0.11982236802577972, 0.11815457791090012, 0.11677553504705429, 0.11567357927560806, 0.11433912813663483]
val_loss [0.28174149990081787, 0.2797890305519104, 0.28358978033065796, 0.27727338671684265, 0.3007770776748657, 0.2775935232639313, 0.28110551834106445, 0.24156667292118073, 0.20867107808589935, 0.3039432168006897, 0.17458944022655487, 0.19854313135147095, 0.17118075489997864, 0.180263951420784, 0.20967070758342743, 0.193481907248497, 0.16275599598884583, 0.24767570197582245, 0.20603327453136444, 0.1946435123682022, 0.23297514021396637, 0.1863243728876114, 0.17451566457748413, 0.24896660447120667, 0.26021212339401245, 0.17278990149497986, 0.14554882049560547, 0.16067463159561157, 0.17398634552955627, 0.2261389046907425, 0.19369851052761078, 0.20536503195762634, 0.1877540796995163, 0.23937729001045227, 0.21908843517303467, 0.19453273713588715, 0.20810450613498688]
